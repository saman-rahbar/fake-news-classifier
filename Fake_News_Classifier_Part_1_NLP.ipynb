{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake News Classifier Using Natural Langage Processing - Part I\n",
    "\n",
    "Libraries used: \n",
    "\n",
    " - NLTK\n",
    " - Gnesim\n",
    " - spaCy\n",
    " - Standford Library for NLTK\n",
    "\n",
    "programming Language:\n",
    " - Pyhton 3.6 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer for text classification:\n",
    "\n",
    "We first load the data into a dataframe. using head() function investigate the columns we can possibly use.\n",
    "\n",
    "Here we use pandas and scikit-learn to create a sparse text vectorizer. Using these tools we can test and train a simple supervised model.\n",
    "\n",
    "We shall now begin by importing the necessary modules in python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# investigating the important columns to use\n",
    "print(df.head())\n",
    "\n",
    "# create a series to sotre the labels: y\n",
    "y = sf.label\n",
    "\n",
    "# creating training and test sets\n",
    "# we split the data based on test_size of 0.33\n",
    "# in order to have the experiment repeatable we chose the random_state to be 53\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[\"text\"], y, test_size = 0.33, random_state = 53)\n",
    "\n",
    "# creating a countvectorizer and removing the stop words\n",
    "count_vectorizer = CountVectorizer(stop_words = \"english\")\n",
    "\n",
    "\n",
    "# transforming the training data using only the 'text' column values:\n",
    "count_train = count_vectorizer.fit_transform(X_train)\n",
    "count_test  = count_vectorizer.transform(X_test)\n",
    "\n",
    "# printing the first 10 features of the count_vectorizer\n",
    "print(count_vectorizer.get_feature_names()[:10])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
